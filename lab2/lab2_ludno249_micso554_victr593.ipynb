{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-03\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Ludvig Noring (ludno249), Michael Sörsäter (micso554), Victor Tranell (victr593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will experiment with $n$-gram models. You will test various parameters that influence these models&rsquo; quality and train to estimate models with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import the Python modules needed for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp2\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab consists of Arthur Conan Doyle&rsquo;s novels about Sherlock Holmes: *The Adventures of Sherlock Holmes*, *The Memoirs of Sherlock Holmes*, *The Return of Sherlock Holmes*, *His Last Bow* and *The Case-Book of Sherlock Holmes*. The next piece of code loads the first three of these as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/advs.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/mems.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/retn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a list of sentences, where one sentence is represented as a list of tokens (strings). The next line prints the 101th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Let', 'us', 'glance', 'at', 'our', 'Continental', 'Gazetteer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and its order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this lab you will examine the relation between an $n$-gram model’s quality and its **order**, i.e. the value of&nbsp;$n$. You will do both a qualitative and quantitative evaluation with the help of the entropy measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line trains a bigram-model of the class `ngrams.Model` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model you are able to generate random sentences. Every time you run the following code cell a new sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" As I came the money in which he see your coffee .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(model.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sentences. Do they sound natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a unigram-, bigram-, trigram-, and quadrigram-model, and generate random sentences with each. How does the quality of the sentences change with the model’s order? Explain your observations using your understanding from how an $n$-gram model works. Use some generated sentences in order to illustrate your discussion. How would the sentences look like for higher values of $n$, such as $n=10$?\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Unigram:\n",
      "Ex1: endeavour the at , believe\n",
      "Ex2: you roamed a was it to , . Quick upon gallop into man's that \" yet and it which Lord I paralyzing of This\n",
      "\n",
      "Traing Bigram:\n",
      "Ex1: \" Fire !\n",
      "Ex2: Then he mounted the queerest club , but is what exact science by trying ones .\n",
      "\n",
      "Traing Trigram:\n",
      "Ex1: Such a charge of conspiracy , as we emerged from the boggy portion of the day in the hall and into his private room , and it was him or he might throw a fiercer light into anything quite so much annoyance upon her husband and child only just returned upon the varnish would have nothing to hide his own bedroom , and a little mystification , Lestrade , grimly .\n",
      "Ex2: \" Can I speak took place .\n",
      "\n",
      "Traing Quadrigram:\n",
      "Ex1: What a charming , old-fashioned room !\n",
      "Ex2: The older man seemed numbed and dazed with a heavy brassy Albert chain , and the gentleman was about to renew her entreaties when a door slammed overhead , and the sun was just grazing the top of a harmonium beside the door .\n",
      "\n",
      "Traing 10-gram model\n",
      "Ex1: It brings me twopence a sheet , and I can often do from fifteen to twenty sheets in a day .\n",
      "Ex2: 2 is an excellent argument with gentlemen who can twist steel pokers into knots .\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "modelNames = [\"Uni\", \"Bi\", \"Tri\", \"Quadri\"]\n",
    "for i, m in enumerate(modelNames):\n",
    "    model = nlp2.train(ngrams.Model, i + 1, training_data)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(\"Traing\", m + \"gram:\")\n",
    "    \n",
    "    print(\"Ex1:\", \" \".join(model.generate()))\n",
    "    print(\"Ex2:\", \" \".join(model.generate()))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "model_10 = nlp2.train(ngrams.Model, 10, training_data)\n",
    "print(\"Traing 10-gram model\")\n",
    "print(\"Ex1:\", \" \".join(model_10.generate()))\n",
    "print(\"Ex2:\", \" \".join(model_10.generate()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n = 1 we simply generate sentences using a \"bag of words-model\". So that could result in the word \"I\" occuring 2 times in a row. Upping the n to 2 that likelihood of that happening is much smaller since I never occurs after I in the sherlock holmes texts. When n grows too large however, say 10, that exact sequence of 9 tokens only occurs one or two times so the model will overfit. *EOS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a quantitative evaluation of a model we can compute its **entropy** on held-out data. We will use the first part of the novel *The Adventures of Sherlock Holmes* for this. It is loaded by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model and computes its entropy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.426862596420277\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)\n",
    "print(\"Entropy:\", nlp2.evaluate(model, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Compute the entropy for the four models you created for the previous problem. How does the model’s entropy change with the model’s order? Explain using your knowledge of the entropy measure.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model.\n",
      "Entropy: 7.338\n",
      "\n",
      "Bigram model.\n",
      "Entropy: 3.427\n",
      "\n",
      "Trigram model.\n",
      "Entropy: 1.429\n",
      "\n",
      "Quadrigram model.\n",
      "Entropy: 0.544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram model.\")\n",
    "    print(\"Entropy: {0:.3f}\".format(nlp2.evaluate(models[i], test_data)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the order increases, the entropy decreases.\n",
    "\n",
    "With a low n-value model, the number of possible outcomes is high and therefore the entropy is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and the estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will implement and evaluate various estimation methods. In order to do that you will need to know how the lab system is built up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the `train()` function (like you did above), the system creates an $n$-gram model of the given class (so far: `ngrams.Model`) and with the given order (the value of $n$) and trains the model on the given data set. For the second part of this lab you will use your own model class. We start with defining the class in such a way that it simply calls the corresponding methods of the superclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def order(self):\n",
    "        \"\"\"Return the order of this model (an integer).\"\"\"\n",
    "        return super().order()\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        \"\"\"Return this model's vocabulary (a set).\"\"\"\n",
    "        return super().vocabulary()\n",
    "    \n",
    "    def freq(self, ctxt, word):\n",
    "        \"\"\"Return the number of occurrences of `word` (a string) after `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().freq(ctxt, word)\n",
    "    \n",
    "    def total(self, ctxt):\n",
    "        \"\"\"Return the total number of ngrams that start with `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().total(ctxt)\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().prob(ctxt, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model of the class `Model` and prints the model’s order (an integer) and the size of its vocabulary (a set of strings, represented by Python’s `set` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of the model: 2\n",
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(\"order of the model:\", model.order())\n",
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up an n-gram’s absolute frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained model consists primarily of a table with absolute frequencies for all $n$-grams that appear in the text it was trained on. In order to look up an $n$-gram’s absolute frequency one can use the method `freq()`. An $n$-gram is divided into two parts: an $(n-1)$-gram called **context** (`ctxt`) and a final unigram (`word`). In Python the context is represented as a tuple of strings and the unigram as a normal string.\n",
    "\n",
    "If you want to train a trigram model and then know the absolute frequency for the trigram *Mr. Sherlock Holmes* you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.freq((\"Mr.\", \"Sherlock\"), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a bigram model and looking up the absolute frequency for the bigram *Baker Street* you can write the following. Note that the context of a bigram model is a 1-tuple of strings, which has a special notation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(model.freq((\"Baker\",), \"Street\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the absolute frequency of an n-gram with a given context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `total()` returns the absolute frequency of $n$-grams with the given context. Here is an example for a trigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.total((\"Mr.\", \"Sherlock\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a bigram model and use it to calculate the following values, using the methods shown above.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** the absolute frequency for the bigram *Sherlock Holmes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "print(model.freq((\"Sherlock\",), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** the absolute frequency of bigrams with the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "print(model.total((\"Sherlock\",)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** the absolute frequency for the unigram *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "model_uni = nlp2.train(Model, 1, training_data)\n",
    "print(model_uni.freq((), \"Sherlock\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.** a list with all the words following the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has Holmes looked ? everywhere ! , . Holmes's\n"
     ]
    }
   ],
   "source": [
    "print(*[word for word in model.vocabulary() if model.prob((\"Sherlock\",), word) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the last exercise you will need to write a bit more than a simple function call.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with the Maximum Likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `prob()` returns the estimated conditional probability $P(w|c)$ for a word $w$ given a context $c$. The following code snippet trains a trigram model and estimates the pobability for *Holmes* given the context *Mr. Sherlock*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What does the returned value imply?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Do your own implementation of the method `prob()`. The method should estimate probabilities using the Maximum Likelihood method. Test your implementation by redoing Exercise&nbsp;2 with the new class; you should get the same result as before. Use the code you wrote in Exercise&nbsp;3 in order to solve the exercise.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model\n",
      "0.005128205128205128\n",
      "\n",
      "Bigram model\n",
      "0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        \n",
    "        return self.freq(ctxt, word) / self.total(ctxt)\n",
    "\n",
    "\n",
    "print(\"Trigram model\")\n",
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.prob((\"Sherlock\",\"Holmes\"), \"I\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Bigram model\")\n",
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(model.prob((\"Sherlock\",), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this exercise you will need to turn the formula for Maximum Likelihood estimation into code. We illustrate the formula for a bigram model. If we write $f(w_1w_2)$ for the number of occurrences of the bigram  $w_1w_2$ and $f(w_1)$ for the number of occurrences of the unigram $w_1$, then the probability for observing $w_2$ given $w_1$ is\n",
    "$$\n",
    "P(w_2|w_1) = \\frac{f(w_1w_2)}{f(w_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Maximum Likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `yoda.txt` contains the same text as `test.txt`, but in the jumbled [Yoda-language]( http://itre.cis.upenn.edu/~myl/languagelog/archives/002173.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yoda_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/yoda.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 5</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation of the four previous models with `yoda.txt` as test data. Something unexpected happens for models with $n>1$. Why? Explain the problem with your knowledge of Maximum Likelihood estimation.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model.\n",
      "Entropy: 7.244\n",
      "\n",
      "Bigram model.\n",
      "I crashed\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram model.\")\n",
    "    try:\n",
    "        print(\"Entropy: {0:.3f}\".format(nlp2.evaluate(models[i], yoda_data)))\n",
    "    except:\n",
    "        print(\"I crashed\")\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the yoda data has scrambled the order of the bigrams has been changed. Thus there exists bigrams in the yoda text that does not exist in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next problem you are going to do Maximum Likelihood estimation, but with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 6</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>\n",
    "Write a new implementation of the method `prob()`, such that it estimates probabilities with additive smoothing.</p>\n",
    "<p>\n",
    "Evaluate the system with new new class using the entropy measure from Problem&nbsp;2. Choose the following values for the smoothing constant $k$:&nbsp;0,00,&nbsp;1,00, 0,10, 0,01. For $k=0$ you should get the same result as in Problem&nbsp;5.\n",
    "</p>\n",
    "<p>\n",
    "How and why does the smoothing constant influence the model’s entropy? Connect to the distribution of the probability mass between observed and imaginary occurrences.\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        \n",
    "        # Some words in the context isn't seen before\n",
    "        for ct in ctxt:\n",
    "            if not ct in self.vocabulary():\n",
    "                return 1 / len(self.vocabulary())\n",
    "        # Word is not seen   \n",
    "        if not word in self.vocabulary():\n",
    "            return (self.k) / (self.total(ctxt) + self.k * len(self.vocabulary()))\n",
    "            \n",
    "                \n",
    "        return (self.freq(ctxt, word) + self.k) / (self.total(ctxt) + self.k * len(self.vocabulary()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "k=0, entropy=7.24\n",
      "k=1, entropy=7.18\n",
      "k=0.1, entropy=7.24\n",
      "k=0.01, entropy=7.24\n",
      "\n",
      "Bigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=7.53\n",
      "k=0.1, entropy=6.27\n",
      "k=0.01, entropy=5.27\n",
      "\n",
      "Trigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=8.76\n",
      "k=0.1, entropy=7.38\n",
      "k=0.01, entropy=5.82\n",
      "\n",
      "Quadrigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=9.05\n",
      "k=0.1, entropy=7.89\n",
      "k=0.01, entropy=6.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_list = [0, 1, 0.10, 0.01]\n",
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram\")\n",
    "    for k in k_list:\n",
    "        model = nlp2.train(Model, i + 1, training_data)    \n",
    "        model.k = k\n",
    "        try:\n",
    "            entropy = nlp2.evaluate(model, yoda_data)\n",
    "        except:\n",
    "            entropy = 0\n",
    "        print('k={k}, entropy={entropy:.2f}'.format(**locals()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table below with your entropy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td></td><td>k = 0.00</td><td>k = 1.00</td><td>k = 0.10</td><td>k = 0.01</td></tr>\n",
    "<tr><td>n = 1</td><td>7.24</td><td>7.18</td><td>7.24</td><td>7.24</td></tr>\n",
    "<tr><td>n = 2</td><td>-</td><td>7.53</td><td>6.27</td><td>5.27</td></tr>\n",
    "<tr><td>n = 3</td><td>-</td><td>8.76</td><td>7.38</td><td>5.82</td></tr>\n",
    "<tr><td>n = 4</td><td>-</td><td>9.05</td><td>7.89</td><td>6.42</td></tr>\n",
    "</table>\n",
    "\n",
    "With k = 1, we smooth the models too much. \n",
    "With the bigram model and k = 0.01 we get the lowest entropy which means it predicts the test data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unseen test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your last exercise is to redo the evaluation on a previously unseen test set, texts from the collection *His Last Bow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseen_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/lstb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 7</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation from Problem 6 with the new test data. Explain what happens given the differences between `test.txt` and `lstb.txt`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=6.26\n",
      "k=0.1, entropy=6.33\n",
      "k=0.01, entropy=6.40\n",
      "\n",
      "Bigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=6.83\n",
      "k=0.1, entropy=5.93\n",
      "k=0.01, entropy=5.55\n",
      "\n",
      "Trigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=8.42\n",
      "k=0.1, entropy=7.70\n",
      "k=0.01, entropy=7.13\n",
      "\n",
      "Quadrigram\n",
      "k=0, entropy=0.00\n",
      "k=1, entropy=8.86\n",
      "k=0.1, entropy=8.48\n",
      "k=0.01, entropy=8.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_list = [0, 1, 0.10, 0.01]\n",
    "\n",
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram\")\n",
    "    for k in k_list:\n",
    "        model = nlp2.train(Model, i + 1, training_data)    \n",
    "        model.k = k\n",
    "        try:\n",
    "            entropy = nlp2.evaluate(model, unseen_data)\n",
    "        except:\n",
    "            entropy = 0\n",
    "        print('k={k}, entropy={entropy:.2f}'.format(**locals()))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unseen data gets higher entropies than the previously seen data for obvious reasons."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
