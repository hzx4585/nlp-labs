{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-03\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Ludvig Noring (ludno249), Michael Sörsäter (micso554), Victor Tranell (victr593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will experiment with $n$-gram models. You will test various parameters that influence these models&rsquo; quality and train to estimate models with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import the Python modules needed for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp2\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab consists of Arthur Conan Doyle&rsquo;s novels about Sherlock Holmes: *The Adventures of Sherlock Holmes*, *The Memoirs of Sherlock Holmes*, *The Return of Sherlock Holmes*, *His Last Bow* and *The Case-Book of Sherlock Holmes*. The next piece of code loads the first three of these as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/advs.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/mems.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/retn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a list of sentences, where one sentence is represented as a list of tokens (strings). The next line prints the 101th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Let', 'us', 'glance', 'at', 'our', 'Continental', 'Gazetteer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and its order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this lab you will examine the relation between an $n$-gram model’s quality and its **order**, i.e. the value of&nbsp;$n$. You will do both a qualitative and quantitative evaluation with the help of the entropy measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line trains a bigram-model of the class `ngrams.Model` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model you are able to generate random sentences. Every time you run the following code cell a new sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come in my mission , put the room with a reward for he was lecturing on the January , with which was a few minutes of experience .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(model.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sentences. Do they sound natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a unigram-, bigram-, trigram-, and quadrigram-model, and generate random sentences with each. How does the quality of the sentences change with the model’s order? Explain your observations using your understanding from how an $n$-gram model works. Use some generated sentences in order to illustrate your discussion. How would the sentences look like for higher values of $n$, such as $n=10$?\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Unigram:\n",
      "Ex1: ? Gradually\n",
      "Ex2: , secret as \" expensive It of eyes the bell-rope not I \" . is is much falling of a good at streets not As a , And . fail out Roylott his ? find you have one a trace us \" severely his keep . a down to had and Colonel my her clear referring From different any For Brunton's , room it leave wrote he men matted\n",
      "\n",
      "Traing Bigram:\n",
      "Ex1: \" he drew up , we call to-morrow .\n",
      "Ex2: \" Oh , in the tendons of course , O , and as to his back upon a window , ' at the law to say into the web to tell me as to his back swiftly down I was waiting for you .\n",
      "\n",
      "Traing Trigram:\n",
      "Ex1: ' \" ' What does he do for the purpose of consulting you .\n",
      "Ex2: \" With a shriek of laughter , I glanced back , and it is always a look at that mark on the sideboard , which shed a feeble light which enabled me to turn to the visit of this gang .\n",
      "\n",
      "Traing Quadrigram:\n",
      "Ex1: Here he is , the more extraordinary , there was always a broken man , kept alive solely through the care of his devoted wife .\n",
      "Ex2: A name derived from the fanciful resemblance to the sound produced by cocking a rifle .\n",
      "\n",
      "Traing 10-gram model\n",
      "Ex1: \" That is Mrs. Toller in the cellar , \" said she .\n",
      "Ex2: Do you tell me that that girl , that angel , is to be tied to Roaring Jack Woodley for life ?\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "modelNames = [\"Uni\", \"Bi\", \"Tri\", \"Quadri\"]\n",
    "for i, m in enumerate(modelNames):\n",
    "    model = nlp2.train(ngrams.Model, i + 1, training_data)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(\"Traing\", m + \"gram:\")\n",
    "    \n",
    "    print(\"Ex1:\", \" \".join(model.generate()))\n",
    "    print(\"Ex2:\", \" \".join(model.generate()))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "model_10 = nlp2.train(ngrams.Model, 10, training_data)\n",
    "print(\"Traing 10-gram model\")\n",
    "print(\"Ex1:\", \" \".join(model_10.generate()))\n",
    "print(\"Ex2:\", \" \".join(model_10.generate()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n = 1 we simply generate sentences using a \"bag of words-model\". So that could result in the word \"I\" occuring 2 times in a row. Upping the n to 2 that likelihood of that happening is much smaller since I never occurs after I in the sherlock holmes texts. When n grows too large however, say 10, that exact sequence of 9 tokens only occurs one or two times so the model will overfit. *EOS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a quantitative evaluation of a model we can compute its **entropy** on held-out data. We will use the first part of the novel *The Adventures of Sherlock Holmes* for this. It is loaded by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model and computes its entropy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.426862596420277\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)\n",
    "print(\"Entropy:\", nlp2.evaluate(model, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Compute the entropy for the four models you created for the previous problem. How does the model’s entropy change with the model’s order? Explain using your knowledge of the entropy measure.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model.\n",
      "Entropy: 7.338\n",
      "\n",
      "Bigram model.\n",
      "Entropy: 3.427\n",
      "\n",
      "Trigram model.\n",
      "Entropy: 1.429\n",
      "\n",
      "Quadrigram model.\n",
      "Entropy: 0.544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram model.\")\n",
    "    print(\"Entropy: {0:.3f}\".format(nlp2.evaluate(models[i], test_data)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the order increases, the entropy decreases.\n",
    "\n",
    "With a low n-value model, the number of possible outcomes is high and therefore the entropy is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and the estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will implement and evaluate various estimation methods. In order to do that you will need to know how the lab system is built up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the `train()` function (like you did above), the system creates an $n$-gram model of the given class (so far: `ngrams.Model`) and with the given order (the value of $n$) and trains the model on the given data set. For the second part of this lab you will use your own model class. We start with defining the class in such a way that it simply calls the corresponding methods of the superclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def order(self):\n",
    "        \"\"\"Return the order of this model (an integer).\"\"\"\n",
    "        return super().order()\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        \"\"\"Return this model's vocabulary (a set).\"\"\"\n",
    "        return super().vocabulary()\n",
    "    \n",
    "    def freq(self, ctxt, word):\n",
    "        \"\"\"Return the number of occurrences of `word` (a string) after `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().freq(ctxt, word)\n",
    "    \n",
    "    def total(self, ctxt):\n",
    "        \"\"\"Return the total number of ngrams that start with `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().total(ctxt)\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().prob(ctxt, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model of the class `Model` and prints the model’s order (an integer) and the size of its vocabulary (a set of strings, represented by Python’s `set` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of the model: 2\n",
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(\"order of the model:\", model.order())\n",
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up an n-gram’s absolute frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained model consists primarily of a table with absolute frequencies for all $n$-grams that appear in the text it was trained on. In order to look up an $n$-gram’s absolute frequency one can use the method `freq()`. An $n$-gram is divided into two parts: an $(n-1)$-gram called **context** (`ctxt`) and a final unigram (`word`). In Python the context is represented as a tuple of strings and the unigram as a normal string.\n",
    "\n",
    "If you want to train a trigram model and then know the absolute frequency for the trigram *Mr. Sherlock Holmes* you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.freq((\"Mr.\", \"Sherlock\"), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a bigram model and looking up the absolute frequency for the bigram *Baker Street* you can write the following. Note that the context of a bigram model is a 1-tuple of strings, which has a special notation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(model.freq((\"Baker\",), \"Street\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the absolute frequency of an n-gram with a given context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `total()` returns the absolute frequency of $n$-grams with the given context. Here is an example for a trigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.total((\"Mr.\", \"Sherlock\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a bigram model and use it to calculate the following values, using the methods shown above.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** the absolute frequency for the bigram *Sherlock Holmes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "print(model.freq((\"Sherlock\",), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** the absolute frequency of bigrams with the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "print(model.total((\"Sherlock\",)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** the absolute frequency for the unigram *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "model_uni = nlp2.train(Model, 1, training_data)\n",
    "print(model_uni.freq((), \"Sherlock\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.** a list with all the words following the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked has everywhere ? ! . Holmes's Holmes ,\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in model.vocabulary() if model.prob((\"Sherlock\",), word) > 0]\n",
    "        \n",
    "print(*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the last exercise you will need to write a bit more than a simple function call.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with the Maximum Likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `prob()` returns the estimated conditional probability $P(w|c)$ for a word $w$ given a context $c$. The following code snippet trains a trigram model and estimates the pobability for *Holmes* given the context *Mr. Sherlock*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What does the returned value imply?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Do your own implementation of the method `prob()`. The method should estimate probabilities using the Maximum Likelihood method. Test your implementation by redoing Exercise&nbsp;2 with the new class; you should get the same result as before. Use the code you wrote in Exercise&nbsp;3 in order to solve the exercise.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model\n",
      "0.005128205128205128\n",
      "\n",
      "Bigram model\n",
      "0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        \n",
    "        return self.freq(ctxt, word)/self.total(ctxt)\n",
    "\n",
    "\n",
    "print(\"Trigram model\")\n",
    "model = nlp2.train(Model, 3, training_data)\n",
    "print(model.prob((\"Sherlock\",\"Holmes\"), \"I\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Bigram model\")\n",
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(model.prob((\"Sherlock\",), \"Holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this exercise you will need to turn the formula for Maximum Likelihood estimation into code. We illustrate the formula for a bigram model. If we write $f(w_1w_2)$ for the number of occurrences of the bigram  $w_1w_2$ and $f(w_1)$ for the number of occurrences of the unigram $w_1$, then the probability for observing $w_2$ given $w_1$ is\n",
    "$$\n",
    "P(w_2|w_1) = \\frac{f(w_1w_2)}{f(w_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Maximum Likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `yoda.txt` contains the same text as `test.txt`, but in the jumbled [Yoda-language]( http://itre.cis.upenn.edu/~myl/languagelog/archives/002173.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yoda_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/yoda.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 5</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation of the four previous models with `yoda.txt` as test data. Something unexpected happens for models with $n>1$. Why? Explain the problem with your knowledge of Maximum Likelihood estimation.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model.\n",
      "Entropy: 7.244\n",
      "\n",
      "Bigram model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-7807afda255a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"gram model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entropy: {0:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoda_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp2/code/nlp2.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_data)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp2/code/ngrams.pyc\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, ctxt, word)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram model.\")\n",
    "    print(\"Entropy: {0:.3f}\".format(nlp2.evaluate(models[i], yoda_data)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the yoda data has scrambled the order of the bigrams has been changed. Thus there exists bigrams in the yoda text that does not exist in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next problem you are going to do Maximum Likelihood estimation, but with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 6</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>\n",
    "Write a new implementation of the method `prob()`, such that it estimates probabilities with additive smoothing.</p>\n",
    "<p>\n",
    "Evaluate the system with new new class using the entropy measure from Problem&nbsp;2. Choose the following values for the smoothing constant $k$:&nbsp;0,00,&nbsp;1,00, 0,10, 0,01. For $k=0$ you should get the same result as in Problem&nbsp;5.\n",
    "</p>\n",
    "<p>\n",
    "How and why does the smoothing constant influence the model’s entropy? Connect to the distribution of the probability mass between observed and imaginary occurrences.\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "k=0, entropy=7.24\n",
      "\n",
      "k=1, entropy=7.18\n",
      "\n",
      "k=0.1, entropy=7.24\n",
      "\n",
      "k=0.01, entropy=7.24\n",
      "\n",
      "Bigram\n",
      "k=0, entropy=0.00\n",
      "\n",
      "k=1, entropy=7.53\n",
      "\n",
      "k=0.1, entropy=6.27\n",
      "\n",
      "k=0.01, entropy=5.27\n",
      "\n",
      "Trigram\n",
      "k=0, entropy=0.00\n",
      "\n",
      "k=1, entropy=8.76\n",
      "\n",
      "k=0.1, entropy=7.38\n",
      "\n",
      "k=0.01, entropy=5.82\n",
      "\n",
      "Quadrigram\n",
      "k=0, entropy=0.00\n",
      "\n",
      "k=1, entropy=9.05\n",
      "\n",
      "k=0.1, entropy=7.89\n",
      "\n",
      "k=0.01, entropy=6.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code\n",
    "        #print(\"MY K IS\", self.k)\n",
    "        return (self.freq(ctxt, word)+self.k)/(self.total(ctxt) + self.k*len(self.vocabulary()))\n",
    "        \n",
    "        return super().prob(ctxt, word)\n",
    "\n",
    "        \n",
    "k_list = [0, 1, 0.10, 0.01]\n",
    "\n",
    "for i, m in enumerate(modelNames):\n",
    "    print(m + \"gram\")\n",
    "    for k in k_list:\n",
    "        model = nlp2.train(Model, i + 1, training_data)    \n",
    "        model.k = k\n",
    "        #print(\"K:\", k, end=\"\\t\")\n",
    "        try:\n",
    "            #print(\"Entropy: {0:.2f}\".format(nlp2.evaluate(model, yoda_data)))\n",
    "            entropy = nlp2.evaluate(model, yoda_data)\n",
    "        except:\n",
    "            #print(\"Entropy: not defined\")\n",
    "            entropy = 0\n",
    "        print('k={k}, entropy={entropy:.2f}'.format(**locals()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table below with your entropy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td></td><td>k = 0,00</td><td>k = 1,00</td><td>k = 0,10</td><td>k = 0,01</td></tr>\n",
    "<tr><td>n = 1</td><td>7.24</td><td>7.18</td><td>7.24</td><td>7.24</td></tr>\n",
    "<tr><td>n = 2</td><td>-</td><td>7.53</td><td>6.27</td><td>5.27</td></tr>\n",
    "<tr><td>n = 3</td><td>-</td><td>8.76</td><td>7.38</td><td>5.82</td></tr>\n",
    "<tr><td>n = 4</td><td>-</td><td>9.05</td><td>7.89</td><td>6.42</td></tr>\n",
    "</table>\n",
    "\n",
    "*TODO: Insert your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unseen test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your last exercise is to redo the evaluation on a previously unseen test set, texts from the collection *His Last Bow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseen_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/lstb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 7</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation from Problem 6 with the new test data. Explain what happens given the differences between `test.txt` and `lstb.txt`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert your explanations here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
