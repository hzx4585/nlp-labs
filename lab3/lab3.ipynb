{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-10\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Ludvig Noring (ludno249), Michael Sörsäter (micso554), Victor Tranell (victr593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the task of labelling words (tokens) with parts of speech such as noun, adjective, and verb. In this lab you will implement a POS tagger based on the averaged perceptron and evaluate it on the [Stockholm Umeå Corpus (SUC)](http://spraakbanken.gu.se/eng/resources/suc), a Swedish corpus containing more than 74,000 sentences (1.1&nbsp;million tokens), which were manually annotated with, among others, parts of speech. The corpus is divided into two files:\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td><code>suc-train.txt</code></td><td style=\"text-align: right\">72,594 sentences</td><td style=\"text-align: right\">1,142,802 tokens</td></tr>\n",
    "<tr><td><code>suc-test.txt</code></td><td style=\"text-align: right\">1,569 sentences</td><td style=\"text-align: right\">23,319 tokens</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the Python module that is required for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = nlp3.read_data(\"/home/TDDE09/labs/nlp3/suc-train.txt\")\n",
    "test_data = nlp3.read_data(\"/home/TDDE09/labs/nlp3/suc-test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets consist of tagged sentences. In Python, a tagged sentence is represented as a list of string pairs, where the first component of each pair represents a word and the second component represents a part-of-speech tag. Run the following code cell to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Och', 'KN'),\n",
       " ('det', 'PN'),\n",
       " ('är', 'VB'),\n",
       " ('som', 'KN'),\n",
       " ('segerherre', 'NN'),\n",
       " ('han', 'PN'),\n",
       " ('vill', 'VB'),\n",
       " ('göra', 'VB'),\n",
       " ('politik', 'NN'),\n",
       " ('.', 'MAD')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell extracts all unique tags from the training data. The tags are explained and exemplified in Table&nbsp;12 (page&nbsp;20) of the [SUC 2.0 Manual](https://spraakbanken.gu.se/parole/Docs/SUC2.0-manual.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB DT HA HD HP HS IE IN JJ KN MAD MID NN PAD PC PL PM PN PP PS RG RO SN UO VB\n"
     ]
    }
   ],
   "source": [
    "suc_tags = set()\n",
    "for tagged_sentence in training_data:\n",
    "    for word, tag in tagged_sentence:\n",
    "        suc_tags.add(tag)\n",
    "suc_tags = sorted(suc_tags)\n",
    "print(\" \".join(suc_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to train the default tagger, tag the sample sentence from above, and evaluate the tagger on the test data. Note that for reasons of speed, this only uses the first 1,000 sentences of the training data; for higher accuracies you should train on the complete training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 15.98%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d60827f3e0ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuc_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {:.2%}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp3/nlp3.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, report_progress)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp3/nlp3.pyc\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, tokens, i, pred_tags)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp3/nlp3.pyc\u001b[0m in \u001b[0;36m_get_features\u001b[0;34m(self, tokens, i, pred_tags)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/nlp3/nlp3.pyc\u001b[0m in \u001b[0;36madd_feature\u001b[0;34m(*args)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tagger = nlp3.PerceptronTagger(suc_tags)\n",
    "tagger.train(training_data)\n",
    "print(tagger.tag([word for word, tag in training_data[42]]))\n",
    "matrix = nlp3.confusion_matrix(tagger, test_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your main task in this lab is to re-implement the two central methods of the default tagger:\n",
    "\n",
    "* `train()`, which takes a list of tagged sentences and trains the tagger using the averaged perceptron learning algorithm\n",
    "\n",
    "* `tag()`, which takes a list of words (strings) and returns a tagged sentence\n",
    "\n",
    "You are of course free to add other methods to your class if you deem it appropriate to do so.\n",
    "\n",
    "In implementing the tagger you will be able to reuse code from your implementation of the averaged perceptron classifier in lab&nbsp;1. However, for this lab it is crucial that you can handle multiple classes, as the tagger needs one class per POS tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a part-of-speech tagger based on the averaged perceptron, train it on the training data, and evaluate performance on the test data. Your tagger should get the same results as the default tagger.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Starter code for this problem is given in the following code cell. The provided class simply inherits from `nlp3.PerceptronTagger` and calls the methods in the superclass. Your task is to replace these calls with your own code. You will note that there is a third method `get_features()`; you do not need to touch this method unless you want to do the advanced part of this lab (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00 %, 88.5 seconds\n",
      "Accuracy: 94.41%\n"
     ]
    }
   ],
   "source": [
    "class OurTagger(nlp3.PerceptronTagger):\n",
    "    def __init__(self, tags):\n",
    "        \"\"\"Creates a new tagger that uses the specified tag set.\"\"\"\n",
    "        super().__init__(tags)\n",
    "        self.tags = tags\n",
    "        self.tag_weights = {}\n",
    "\n",
    "            \n",
    "    def tag(self, words):\n",
    "        \"\"\"Tags the specified words, returning a tagged sentence.\"\"\"\n",
    "        predicted_tags = []\n",
    "        prev_tag = ''\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            features = self.get_features(words, i, predicted_tags)\n",
    "            best_score = -999\n",
    "            predicted_tag = 'none'\n",
    "\n",
    "            for current_tag in self.tag_weights:\n",
    "                tmp_score = 0\n",
    "                for feature in features:\n",
    "                    if feature not in self.tag_weights['NN']:\n",
    "                        continue\n",
    "                    \n",
    "                    tmp_score += self.tag_weights[current_tag][feature]\n",
    "                if tmp_score > best_score:\n",
    "                    best_score = tmp_score\n",
    "                    predicted_tag = current_tag\n",
    "        \n",
    "            if predicted_tag == 'none':\n",
    "                print(words)\n",
    "                print(word)\n",
    "                predicted_tag = 'NN'\n",
    "            predicted_tags.append(predicted_tag)\n",
    "\n",
    "        return list(zip(words, predicted_tags))\n",
    "    \n",
    "    def train(self, tagged_sentences, report_progress=True):\n",
    "        import time\n",
    "        \"\"\"Trains this tagger on the specified gold-standard data.\"\"\"\n",
    "        #super().train(tagged_sentences, report_progress)\n",
    "        #return\n",
    "        t0 = time.time()\n",
    "        total_count = len(tagged_sentences)\n",
    "\n",
    "        # weight vectors for the different classes\n",
    "        acc = {}\n",
    "        for tag in self.tags:\n",
    "            self.tag_weights[tag] = {}\n",
    "            acc[tag] = {}\n",
    "\n",
    "        acc_cnt = 1\n",
    "        # loop over training data\n",
    "        progress_cnt = 0\n",
    "        for sentence in tagged_sentences:\n",
    "            progress_cnt += 1\n",
    "            print(\"Progress: {0:.2f} %, {1:.1f} seconds\".format(100 * progress_cnt / total_count, time.time()-t0), end=\"\\r\")\n",
    "\n",
    "            tokens = [token for token, tag in sentence]\n",
    "            pred_tags = []\n",
    "            for i, pair in enumerate(sentence):\n",
    "                correct_tag = pair[1]\n",
    "                predicted_tag = 'NN'\n",
    "                best_score = -999                \n",
    "                features = self.get_features(tokens, i, pred_tags)\n",
    "                \n",
    "                #pred_tags.append(correct_tag)\n",
    "                \n",
    "                # Add features too dicts\n",
    "                for feature in features:\n",
    "                    if feature not in self.tag_weights['NN']:\n",
    "                        for tag in self.tag_weights:\n",
    "                            self.tag_weights[tag][feature] = 0\n",
    "                            acc[tag][feature] = 0 # add the tag to the acc\n",
    "\n",
    "                for current_tag in self.tag_weights:\n",
    "                    tmp_score = 0\n",
    "                    for feature in features:\n",
    "                        tmp_score += self.tag_weights[current_tag][feature] \n",
    "                    if(tmp_score > best_score):\n",
    "                        predicted_tag = current_tag\n",
    "                        best_score = tmp_score\n",
    "\n",
    "                if correct_tag != predicted_tag:\n",
    "                    for feature in features:\n",
    "                        self.tag_weights[predicted_tag][feature] -= 1\n",
    "                        self.tag_weights[correct_tag][feature] += 1\n",
    "\n",
    "                        acc[predicted_tag][feature] -= acc_cnt\n",
    "                        acc[correct_tag][feature] += acc_cnt\n",
    "                pred_tags.append(predicted_tag)\n",
    "                acc_cnt += 1\n",
    "\n",
    "        # Averaging\n",
    "        #print(self.tag_weights['NN'])\n",
    "        for current_tag in self.tag_weights:\n",
    "            for feature in self.tag_weights[current_tag]:\n",
    "                self.tag_weights[current_tag][feature] -= acc[current_tag][feature] / acc_cnt\n",
    "\n",
    "        print()\n",
    "        \n",
    "        #print(self.tag_weights['NN'])\n",
    "\n",
    "    def get_features(self, tokens, i, pred_tags):\n",
    "        \"\"\"Extracts the feature list for the specified configuration.\"\"\"\n",
    "        \n",
    "        # Tuning parameters\n",
    "        t_word = 4 #4\n",
    "        t_prevtag = 2 #2\n",
    "        t_nextword = 2 #2\n",
    "        t_prevword = 2 #2\n",
    "        t_wordlen = 1 #1\n",
    "        t_prevlen = 1 #1\n",
    "        t_case = 1 #1\n",
    "        t_currend = 0 # 0\n",
    "        \n",
    "        featurelist = []        \n",
    "        featurelist += [tokens[i]] * t_word\n",
    "        \n",
    "\n",
    "        if i == 0:            \n",
    "            featurelist += [\"prevtag:<BOS>\"] * t_prevtag\n",
    "        else:\n",
    "            featurelist += [\"prevtag:\"+str(pred_tags[-1])] * t_prevtag\n",
    "        \n",
    "        if i == len(tokens)-1:\n",
    "            featurelist += [\"nextword:<EOS>\"] * t_nextword\n",
    "        else:\n",
    "            featurelist += [\"nextword:\"+tokens[i+1]] * t_nextword\n",
    "            \n",
    "        if i == 0:\n",
    "            featurelist += ['prevword:<BOS>'] * t_prevword\n",
    "            \n",
    "        else:\n",
    "            featurelist += ['prevword:'+str(tokens[i-1])] * t_prevword\n",
    "                \n",
    "        featurelist += ['wordlen:'+str(len(tokens[i]))] * t_wordlen\n",
    "        \n",
    "        if i == 0:\n",
    "            featurelist += ['prevlen:0'] * t_prevlen\n",
    "        else:\n",
    "            featurelist += ['prevlen:'+str(len(tokens[i-1]))] * t_prevlen\n",
    "            \n",
    "        if tokens[i][0].isupper():# and i > 0:\n",
    "            featurelist += ['case:upper'] * t_case\n",
    "        else:\n",
    "            featurelist += ['case:lower'] * t_case\n",
    "        \n",
    "        if not i == 0:\n",
    "            featurelist += [\"currend:\" + tokens[i][-1:-3]] * t_currend\n",
    "                \n",
    "        return featurelist\n",
    "        #return [tokens[i]]\n",
    "        #return super().get_features(tokens, i, pred_tags)\n",
    "        \n",
    "our_tagger = OurTagger(suc_tags)\n",
    "our_tagger.train(training_data[:])\n",
    "our_matrix = nlp3.confusion_matrix(our_tagger, test_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(our_matrix)))\n",
    "\n",
    "#vanlig\n",
    "#10 000 - 90.16\n",
    "\n",
    "#i > 0 på upper\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test your tagger. At the end of the lab you should get the same results as in the evaluation of the default tagger (assuming that you do not change the feature extraction, see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00 %, 12.3 seconds\n",
      "Accuracy: 90.16%\n"
     ]
    }
   ],
   "source": [
    "our_tagger = OurTagger(suc_tags)\n",
    "our_tagger.train(training_data[:10000])\n",
    "our_matrix = nlp3.confusion_matrix(our_tagger, test_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(our_matrix)))\n",
    "\n",
    "# 75.96 - 1000\n",
    "# 92.35 - alla\n",
    "# vår\n",
    "# 76.52 - 1000\n",
    "# 92.6 - alla\n",
    "\n",
    "# FEATURES\n",
    "# std: 76.52 87.26\n",
    "# nuvarande ord: 64.22\n",
    "\n",
    "\n",
    "# bos, längd av ord: 74.48\n",
    "# föregående ord, föregående klass, ordlängd, föregående ordlängd, stor bokstav, ändelse: 74.51 \n",
    "# föregående ord, föregående klass, ordlängd, föregående ordlängd, stor bokstav, ord igen: 77.26 86.69\n",
    "\n",
    "\n",
    "\n",
    "# 10 000\n",
    "# std: 87.26\n",
    "# alla: 89.12\n",
    "# 90.11\n",
    "\n",
    "# 94.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we try to give you an idea of what the two methods `train()` and `tag()` do. We start with the latter.\n",
    "\n",
    "### Tagging\n",
    "\n",
    "The default tagger implements the sequence model presented in the lecture. In this model, sentences are tagged from left to right. A **configuration** consists of the list of words, the index of the current word, and the list of already predicted tags. For each word in the sentence, the tagger calls the method `get_features()` to obtain a feature vector for the current configuration. To illustrate how this works, we define a variant of the default tagger that only extracts a single feature, the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DemoTagger(nlp3.PerceptronTagger):\n",
    "    \n",
    "    def get_features(self, words, i, pred_tags):\n",
    "        features = [words[i]]\n",
    "        if self.debug:\n",
    "            print(\"words: {}\".format(\" \".join(words)))\n",
    "            print(\"i: {} (current word: {})\".format(i, words[i]))\n",
    "            print(\"pred_tags: {}\".format(\" \".join(pred_tags)))\n",
    "            print(\"features: {}\".format(\" \".join(features)))\n",
    "            print()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this tagger and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_tagger = DemoTagger(suc_tags)\n",
    "demo_tagger.debug = False\n",
    "demo_tagger.train(training_data[:1000])\n",
    "demo_matrix = nlp3.confusion_matrix(demo_tagger, test_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(demo_matrix)))\n",
    "demo_tagger.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the features that are extracted when the system tags the sentence *Anna älskar Kurt*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_tagger.tag(\"Anna älskar Kurt\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a feature vector is represented as a list of features. With this vector, the tagger then predicts the next tag using the classification rule for the perceptron, and updates the configuration before moving on to the next word. Finally, `tag()` returns the tagged sentence.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training is based on the learning algorithm for the averaged perceptron. Note that the weight vectors need to be updated for each word, not for each sentence. The tagger maintains a list of already predicted tags as part of its configuration. The tagger trains for a single epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the advanced part of this lab, you will practice your skills in **feature engineering**, the task of identifying useful features for a machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Think about which features could be useful for tagging and re-implement the method `get_features()` in the class `OurTagger` accordingly. Experiment not only with atomic features but also with different feature combinations (pairs or tuples of features). The goal is to create a system whose accuracy on the test data is as high as possible. For full credit you will have to achieve an accuracy of at least 93% on the test data. Provide a short description of how you came up with your features.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we tried to copy the feature set shown during the lectures. We got that to work equally as good as the provided tagger with the original super.get_feature(). After that we tested every features we could think of one by one to figure out if they made any imporvement. When we had about 6 features we started tuning them. By adding the same feature multiple times we could put a heavier weight on those features. When we could not think of any more features or could not find any better combination of weights we considered ourselves finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
