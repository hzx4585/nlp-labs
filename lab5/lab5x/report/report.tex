\documentclass[12pt,a4paper]{article}
\usepackage[a4paper,textwidth=170mm,textheight=245mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{datetime2}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{float}

\pagestyle{fancy}
\fancyhf{}

\lhead{TDDE09 Natural Language Processing}
\rhead{\today}
\setlength{\headheight}{15pt}

\pagenumbering{gobble}

\cfoot{\thepage}

\begin{document}

\begin{center}

	\vspace{0.7cm}
	\LARGE
	\textbf{Lab 5X report }

	\Large
	\begin{table}[h]
	\centering
		\begin{tabular}{ r l }
		Ludvig Noring & ludno249 \\ Michael Sörsäter & micso554 \\ Victor Tranell &  victr593 \\
		\end{tabular}
	\end{table}
\end{center}

\subsection*{Test data}
After taking the test we got 83 \% accuracy.
\subsection*{Word space model}
We train the word space model with the two given files.
Compute the similarity between the gold standard word and all the candidates.
The candidate with the highest score is most likely the synonym.

\subsection*{Evaluation}
The performance of our two models:

\begin{table}[H]
	\centering
	\begin{tabular}{c c c c}
		File & \#words & Training time & Accuracy \\ \hline
		oanc.txt & 11.4 million & 1.5 minute & 58.75 \% \\
		enwik9.txt & 124.3 million & 31 minutes & 77.50 \%\\
	\end{tabular}
\end{table}

The first model, OANC, got an lower accuracy than a non-native english speaker.
However when training on the entire english wikipedia the model performed better than the non-native speakers.
We still performed better :)

\end{document}
